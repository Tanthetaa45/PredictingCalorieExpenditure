


import pandas as pd
import os


DATA_PATH = "/Volumes/Extreme SSD/calorieEstimator/data/data"


train_df = pd.read_csv(os.path.join(DATA_PATH, "train.csv"))
test_df = pd.read_csv(os.path.join(DATA_PATH, "test.csv"))
sample_submission = pd.read_csv(os.path.join(DATA_PATH, "sample_submission.csv"))


print("Train Shape:", train_df.shape)
print("Test Shape:", test_df.shape)
print("Sample Submission Shape:", sample_submission.shape)


print("\nTrain Head:")
print(train_df.head())


print("\nMissing Values:")
print(train_df.isnull().sum())


print("\nData Types:")
print(train_df.dtypes)



import sklearn
from sklearn.metrics import mean_squared_error
import inspect

print("scikit-learn version:", sklearn.__version__)
print("mean_squared_error path:", inspect.getfile(mean_squared_error))


!pip install seaborn


import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder, StandardScaler, KBinsDiscretizer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

from catboost import CatBoostRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor


import warnings
warnings.filterwarnings("ignore")


print(train_df.describe())
print(test_df.describe())


train_df=train_df.drop_duplicates()
print(train_df)



train_df.info()
train_df.nunique()


#EDA PLOTS AND GRAPHS FOR STATISTICS 
fig,axis=plt.subplots(nrows=7,ncols=1,figsize=(8,20))
for i,col in enumerate(train_df.select_dtypes(include=[np.number]).columns):
    sns.histplot(train_df[col],bins=30,kde=True,ax=axis[i])
    axis[i].set_title(f'Histogram of {col}')
plt.tight_layout()
plt.show()


fig, axis = plt.subplots(nrows=7, ncols=2, figsize=(12, 10))


axis = axis.reshape(7, 2)


numeric_cols = train_df.select_dtypes(include=[np.number]).drop('Calories', axis=1).columns

for i, feature in enumerate(numeric_cols):
    row = i // 2
    col_idx = i % 2
    sns.violinplot(x=train_df['Sex'], y=train_df[feature], ax=axis[row, col_idx])
    axis[row, col_idx].set_title(f'Sex vs {feature}')

plt.tight_layout()
plt.show()


sns.heatmap(train_df.corr(numeric_only=True),annot=True,cmap='coolwarm')
plt.title("Correlation HeatMap")
plt.show()




sns.pairplot(train_df)
plt.suptitle("Pairplot of All Features",y=1.02)
plt.show()





from sklearn.preprocessing import LabelEncoder
import numpy as np


label_enc = LabelEncoder()
label_enc.fit(train_df['Sex'])


train_df['Sex'] = label_enc.transform(train_df['Sex'])
test_df['Sex'] = label_enc.transform(test_df['Sex'])


def feature_engineering(data, numeric_cols):
    data['BMI'] = data['Weight'] / (data['Height'] / 100)**2
    data['Intensity'] = data['Heart_Rate'] / data['Duration']
    for i in range(len(numeric_cols)):
        f1 = numeric_cols[i]
        for j in range(i+1, len(numeric_cols)):
            f2 = numeric_cols[j]
            data[f'{f1}_x_{f2}'] = data[f1] * data[f2]
    return data


numeric_cols = ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']


train_df = feature_engineering(train_df, numeric_cols)
test_df = feature_engineering(test_df, numeric_cols)


X = train_df.drop(['Calories'], axis=1)
y = np.log1p(train_df['Calories'].values)





pca=PCA(n_components=3)
X_pca=pca.fit_transform(X.drop('Sex',axis=1))
kmeans_per_k=[KMeans(n_clusters=k,random_state=42).fit(X_pca) for k in range(1,10)]
inertias=[model.inertia_ for model in kmeans_per_k]
plt.plot(range(1,10),inertias,"bo-")
plt.xlabel("$k$")
plt.ylabel("Inertia")
plt.grid(True)
plt.title("Elbow Method for Optimal K")
plt.show()
kmeans=KMeans(n_clusters=3,random_state=42).fit(X_pca)
pca_df=pd.DataFrame(X_pca,columns=['PCA1','PCA2','PCA3'])
pca_df['cluster']=kmeans.labels_
pca_df['target_group']=pd.qcut(train_df['Calories'],q=3,labels=False)
accuracy=(pca_df['cluster']==pca_df['target_group']).mean()
print(f"Accuracy between clusters and target groups:{accuracy:.2%}")
fig = plt.figure(figsize=(10,8))
ax = fig.add_subplot(111, projection='3d')

scatter = ax.scatter(pca_df['PCA1'], pca_df['PCA2'], pca_df['PCA3'], c=pca_df['target_group'], cmap='viridis', alpha=0.7)
plt.title("3D PCA Projection with Target Group Coloring")
plt.colorbar(scatter, label='Target Group')
plt.show()
                               


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
from mpl_toolkits.mplot3d import Axes3D  # Needed for 3D plotting

# Step 1: Drop non-numeric column and scale
X_features = X.drop('Sex', axis=1)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_features)

# Step 2: Apply PCA
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_scaled)

# Step 3: Elbow method to find optimal k
kmeans_per_k = [KMeans(n_clusters=k, random_state=42, n_init=10).fit(X_pca) for k in range(1, 10)]
inertias = [model.inertia_ for model in kmeans_per_k]

plt.plot(range(1, 10), inertias, "bo-")
plt.xlabel("$k$")
plt.ylabel("Inertia")
plt.grid(True)
plt.title("Elbow Method for Optimal K")
plt.show()

# Step 4: Fit final KMeans with k=3 (or based on elbow)
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10).fit(X_pca)

# Step 5: Create DataFrame with PCA and clustering info
pca_df = pd.DataFrame(X_pca, columns=['PCA1', 'PCA2', 'PCA3'])
pca_df['cluster'] = kmeans.labels_
pca_df['target_group'] = pd.qcut(train_df['Calories'], q=3, labels=False)

# Step 6: Evaluate clustering performance
ari = adjusted_rand_score(pca_df['target_group'], pca_df['cluster'])
nmi = normalized_mutual_info_score(pca_df['target_group'], pca_df['cluster'])

print(f"Adjusted Rand Index (ARI): {ari:.2%}")
print(f"Normalized Mutual Information (NMI): {nmi:.2%}")

# Step 7: 3D PCA plot colored by target group
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(pca_df['PCA1'], pca_df['PCA2'], pca_df['PCA3'],
                     c=pca_df['target_group'], cmap='viridis', alpha=0.7)
ax.set_title("3D PCA Projection with Target Group Coloring")
plt.colorbar(scatter, label='Target Group')
plt.show()

# Step 8: 3D PCA plot colored by cluster
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(pca_df['PCA1'], pca_df['PCA2'], pca_df['PCA3'],
                     c=pca_df['cluster'], cmap='tab10', alpha=0.7)
ax.set_title("3D PCA Projection with KMeans Cluster Coloring")
plt.colorbar(scatter, label='Cluster')
plt.show()








pip install lightgbm


import numpy as np
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from catboost import CatBoostRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor, early_stopping
import lightgbm as lgbm

# Assuming X is a DataFrame and y is a NumPy array
FOLDS = 2
kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)

cat_preds = np.zeros((FOLDS, len(test_df)))
xgb_preds = np.zeros((FOLDS, len(test_df)))
lgbm_preds = np.zeros((FOLDS, len(test_df)))

oof_cat = np.zeros(len(X))
oof_xgb = np.zeros(len(X))
oof_lgbm = np.zeros(len(X))

for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):
    print(f"\n{'#'*15} Fold {fold+1} {'#'*15}")
    
    X_train, y_train = X.iloc[train_idx], y[train_idx]
    X_valid, y_valid = X.iloc[valid_idx], y[valid_idx]

    # CatBoost
    cat_model = CatBoostRegressor(
        iterations=3500,
        learning_rate=0.02,
        depth=12,
        l2_leaf_reg=3,
        verbose=1000,
        early_stopping_rounds=200,
        loss_function='RMSE',
        eval_metric='RMSE',
        task_type='CPU'
    )
    cat_model.fit(X_train, y_train, eval_set=(X_valid, y_valid), use_best_model=True, cat_features=[0])  # Update cat_features as needed

    # XGBoost
    xgb_model = XGBRegressor(
        max_depth=10,
        n_estimators=2000,
        learning_rate=0.01,
        subsample=0.9,
        colsample_bytree=0.75,
        gamma=0.01,
        enable_categorical=True,
        tree_method="hist",
        device="cpu",
        eval_metric="rmse",               # ✅ Must be in constructor
        early_stopping_rounds=100,        # ✅ Must be in constructor (v3.0+)
        verbosity=0                       # Silent mode
    )
    xgb_model.fit(
        X_train,
        y_train,
        eval_set=[(X_valid, y_valid)]
    )

    # LightGBM
    lgbm_model = LGBMRegressor(
        num_leaves=50,
        max_depth=10,
        learning_rate=0.01,
        n_estimators=3000,
        subsample=0.8,
        colsample_bytree=0.75,
        reg_alpha=1,
        reg_lambda=1,
        verbose=-1
    )
    lgbm_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], callbacks=[early_stopping(100)])

    # OOF Predictions
    oof_cat[valid_idx] = cat_model.predict(X_valid)
    oof_xgb[valid_idx] = xgb_model.predict(X_valid)
    oof_lgbm[valid_idx] = lgbm_model.predict(X_valid)

    # Test Predictions
    cat_preds[fold] = cat_model.predict(test_df)
    xgb_preds[fold] = xgb_model.predict(test_df)
    lgbm_preds[fold] = lgbm_model.predict(test_df)

    # Fold-wise RMSE
    cat_rmse = mean_squared_error(y_valid, oof_cat[valid_idx], squared=False)
    xgb_rmse = mean_squared_error(y_valid, oof_xgb[valid_idx], squared=False)
    lgbm_rmse = mean_squared_error(y_valid, oof_lgbm[valid_idx], squared=False)

    print(f'CAT_RMSE: {cat_rmse:.4f}, XGB_RMSE: {xgb_rmse:.4f}, LGBM_RMSE: {lgbm_rmse:.4f}')

# Average predictions across folds
pred_cat = np.expm1(np.mean(cat_preds, axis=0))
pred_xgb = np.expm1(np.mean(xgb_preds, axis=0))
pred_lgbm = np.expm1(np.mean(lgbm_preds, axis=0))

# Weighted Ensemble
final_pred = pred_cat * 0.3 + pred_xgb * 0.3 + pred_lgbm * 0.4
final_pred = np.clip(final_pred, 1, 314)


import xgboost
print(xgboost.__version__)
import sklearn
print(sklearn.__version__)
import sklearn
import inspect
from sklearn.metrics import mean_squared_error

print("scikit-learn version:", sklearn.__version__)

print("Location of mean_squared_error:", inspect.getfile(mean_squared_error))
from sklearn.metrics import mean_squared_error
import inspect
print(inspect.getfile(mean_squared_error))
